{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alphapose command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Input dir: Run AlphaPose for all images in a folder with:\n",
    "python scripts/demo_inference.py --cfg <cfg_file> --checkpoint <trained_model> --indir <img_directory> --outdir <output_directory>\n",
    "\n",
    "* Choose a different detector: Default detector is yolov3-spp, it works pretty well, if you want to use yolox series, remember to download their weight according to our installation readme. Options include [yolox-x|yolox-l|yolox-m|yolox-s|yolox-darknet]:\n",
    "python scripts/demo_inference.py --detector yolox-x --cfg <cfg_file> --checkpoint <trained_model> --indir <img_directory> --outdir <output_directory>\n",
    "\n",
    "* Video: Run AlphaPose for a video and save the rendered video with:\n",
    "python scripts/demo_inference.py --cfg configs/halpe_26/resnet/256x192_res50_lr1e-3_2x.yaml --checkpoint pretrained_models/halpe26_fast_res50_256x192.pth --video examples/video/cjs_exam.mp4 --outdir examples/results --save_video\n",
    "\n",
    "* Webcam: Run AlphaPose using default webcam and visualize the results with:\n",
    "python scripts/demo_inference.py --cfg <cfg_file> --checkpoint <trained_model> --outdir examples/res --vis --webcam 0\n",
    "\n",
    "* Input list: Run AlphaPose for images in a list and save the rendered images with:\n",
    "python scripts/demo_inference.py --cfg <cfg_file> --checkpoint <trained_model> --list examples/list-coco-demo.txt --indir <img_directory> --outdir examples/res --save_img\n",
    "\n",
    "* Only-cpu/Multi-gpus: Run AlphaPose for images in a list by cpu only or multi gpus:\n",
    "python scripts/demo_inference.py --cfg <cfg_file> --checkpoint <trained_model> --list examples/list-coco-demo.txt --indir <img_directory> --outdir examples/res --gpus ${-1(cpu only)/0,1,2,3(multi-gpus)}\n",
    "\n",
    "* Re-ID Track(Experimental): Run AlphaPose for tracking persons in a video by human re-id algorithm:\n",
    "python scripts/demo_inference.py --cfg <cfg_file> --checkpoint <trained_model> --video <path to video> --outdir examples/res --pose_track --save_video\n",
    "\n",
    "* Simple Track(Experimental): Run AlphaPose for tracking persons in a video by MOT tracking algorithm:\n",
    "python scripts/demo_inference.py --cfg <cfg_file> --checkpoint <trained_model> --video <path to video> --outdir examples/res --detector tracker --save_video\n",
    "\n",
    "* Pose Flow(not ready): Run AlphaPose for tracking persons in a video by embedded PoseFlow algorithm:\n",
    "python scripts/demo_inference.py --cfg <cfg_file> --checkpoint <trained_model. --video <path to video> --outdir examples/res --pose_flow --save_video\n",
    "\n",
    "* Note: If you meet OOM(out of memory) problem, decreasing the pose estimation batch until the program can run on your computer:\n",
    "python scripts/demo_inference.py --cfg <cfg_file> --checkpoint <trained_model> --indir <img_directory> --outdir examples/res --detbatch 1 --posebatch 30\n",
    "\n",
    "* Getting more accurate: You can use larger input for pose network to improve performance e.g.:\n",
    "python scripts/demo_inference.py --cfg <cfg_file> --checkpoint <trained_model> --indir <img_directory> --outdir <output_directory> --flip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motionbert command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 3d pose\n",
    "python infer_wild.py \\\n",
    "--vid_path examples/AlphaPose_cjs_exam.mp4 \\\n",
    "--json_path examples/alphapose-results.json \\\n",
    "--out_path examples/results\n",
    "\n",
    "* mesh\n",
    "python infer_wild_mesh.py \\\n",
    "--vid_path examples/AlphaPose_cjs_exam.mp4 \\\n",
    "--json_path examples/alphapose-results.json \\\n",
    "--out_path examples/results \\\n",
    "--ref_3d_motion_path examples/results/X3D.npy(option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Webcam index 0 is not available.\n",
      "Webcam index 1 is not available.\n",
      "Webcam index 2 is not available.\n",
      "Webcam index 3 is not available.\n",
      "Webcam index 4 is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@9418.283] global cap_v4l.cpp:997 open VIDEOIO(V4L2:/dev/video0): can't open camera by index\n",
      "[ERROR:0@9418.283] global obsensor_uvc_stream_channel.cpp:159 getStreamChannelGroup Camera index out of range\n",
      "[ WARN:0@9418.286] global cap_v4l.cpp:997 open VIDEOIO(V4L2:/dev/video1): can't open camera by index\n",
      "[ERROR:0@9418.286] global obsensor_uvc_stream_channel.cpp:159 getStreamChannelGroup Camera index out of range\n",
      "[ WARN:0@9418.288] global cap_v4l.cpp:997 open VIDEOIO(V4L2:/dev/video2): can't open camera by index\n",
      "[ERROR:0@9418.288] global obsensor_uvc_stream_channel.cpp:159 getStreamChannelGroup Camera index out of range\n",
      "[ WARN:0@9418.289] global cap_v4l.cpp:997 open VIDEOIO(V4L2:/dev/video3): can't open camera by index\n",
      "[ERROR:0@9418.289] global obsensor_uvc_stream_channel.cpp:159 getStreamChannelGroup Camera index out of range\n",
      "[ WARN:0@9418.291] global cap_v4l.cpp:997 open VIDEOIO(V4L2:/dev/video4): can't open camera by index\n",
      "[ERROR:0@9418.291] global obsensor_uvc_stream_channel.cpp:159 getStreamChannelGroup Camera index out of range\n"
     ]
    }
   ],
   "source": [
    "# 카메라 연결 확인 코드\n",
    "import cv2\n",
    "\n",
    "for i in range(5):\n",
    "    cap = cv2.VideoCapture(i)\n",
    "    if cap.isOpened():\n",
    "        print(f\"Webcam index {i} is available.\")\n",
    "        cap.release()\n",
    "    else:\n",
    "        print(f\"Webcam index {i} is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(629, 17, 3)\n",
      "[[[-0.1434395   0.16969839  0.        ]\n",
      "  [-0.25752604  0.18429506 -0.01000124]\n",
      "  [-0.24302873  0.5482167   0.08387277]\n",
      "  ...\n",
      "  [-0.3006009  -0.34079653 -0.07275216]\n",
      "  [-0.35476    -0.09653915  0.01704303]\n",
      "  [-0.33017528  0.08956937 -0.13777421]]\n",
      "\n",
      " [[-0.14331692  0.17023128  0.00181606]\n",
      "  [-0.25741172  0.18510553 -0.00931405]\n",
      "  [-0.24157628  0.5480118   0.08331447]\n",
      "  ...\n",
      "  [-0.30294955 -0.33693945 -0.07028489]\n",
      "  [-0.35721135 -0.09093743  0.02399519]\n",
      "  [-0.33727947  0.09994338 -0.12779072]]\n",
      "\n",
      " [[-0.14296311  0.17066865  0.00175973]\n",
      "  [-0.25721788  0.18584898 -0.00855563]\n",
      "  [-0.24010554  0.54838955  0.08439484]\n",
      "  ...\n",
      "  [-0.3054209  -0.3323692  -0.06785031]\n",
      "  [-0.3571838  -0.08278228  0.02912389]\n",
      "  [-0.34230596  0.1137187  -0.11372767]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-0.151198    0.23738626  0.00505891]\n",
      "  [-0.17300315  0.27391234 -0.10210109]\n",
      "  [-0.14218947  0.6555146   0.1102583 ]\n",
      "  ...\n",
      "  [-0.23638639 -0.35786542 -0.19325882]\n",
      "  [-0.2250172   0.01638646 -0.22114363]\n",
      "  [-0.10014153  0.30553496 -0.20961998]]\n",
      "\n",
      " [[-0.15013331  0.23647307  0.00508072]\n",
      "  [-0.17340869  0.2730133  -0.10170763]\n",
      "  [-0.14338073  0.6560662   0.11052389]\n",
      "  ...\n",
      "  [-0.24127139 -0.3620065  -0.19311243]\n",
      "  [-0.23232342  0.01391562 -0.22208707]\n",
      "  [-0.1075419   0.30522496 -0.21136683]]\n",
      "\n",
      " [[-0.14925678  0.236158    0.0050124 ]\n",
      "  [-0.17361201  0.27304584 -0.10112146]\n",
      "  [-0.14443132  0.6565249   0.11065149]\n",
      "  ...\n",
      "  [-0.24544264 -0.3656631  -0.19275185]\n",
      "  [-0.23943853  0.01133628 -0.22304836]\n",
      "  [-0.11497398  0.3047352  -0.21383482]]]\n"
     ]
    }
   ],
   "source": [
    "# 3d pose 결과 numpy 파일 내용 확인 코드\n",
    "import numpy as np\n",
    "\n",
    "# npy 디렉토리\n",
    "file_path = 'MotionBERT/examples/results/X3D.npy'\n",
    "\n",
    "# npy 파일 읽기\n",
    "data = np.load(file_path)\n",
    "\n",
    "# 데이터 확인\n",
    "print(data.shape) # (프레임 개수, 키포인트, (x,y,z) 좌표) \n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sjd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
