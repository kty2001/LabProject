{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# cuda 연결 확인 코드\n",
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Webcam index 0 is not available.\n",
      "Webcam index 1 is not available.\n",
      "Webcam index 2 is not available.\n",
      "Webcam index 3 is not available.\n",
      "Webcam index 4 is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@6767.727] global cap_v4l.cpp:997 open VIDEOIO(V4L2:/dev/video0): can't open camera by index\n",
      "[ERROR:0@6767.727] global obsensor_uvc_stream_channel.cpp:159 getStreamChannelGroup Camera index out of range\n",
      "[ WARN:0@6767.729] global cap_v4l.cpp:997 open VIDEOIO(V4L2:/dev/video1): can't open camera by index\n",
      "[ERROR:0@6767.729] global obsensor_uvc_stream_channel.cpp:159 getStreamChannelGroup Camera index out of range\n",
      "[ WARN:0@6767.729] global cap_v4l.cpp:997 open VIDEOIO(V4L2:/dev/video2): can't open camera by index\n",
      "[ERROR:0@6767.729] global obsensor_uvc_stream_channel.cpp:159 getStreamChannelGroup Camera index out of range\n",
      "[ WARN:0@6767.730] global cap_v4l.cpp:997 open VIDEOIO(V4L2:/dev/video3): can't open camera by index\n",
      "[ERROR:0@6767.730] global obsensor_uvc_stream_channel.cpp:159 getStreamChannelGroup Camera index out of range\n",
      "[ WARN:0@6767.730] global cap_v4l.cpp:997 open VIDEOIO(V4L2:/dev/video4): can't open camera by index\n",
      "[ERROR:0@6767.730] global obsensor_uvc_stream_channel.cpp:159 getStreamChannelGroup Camera index out of range\n"
     ]
    }
   ],
   "source": [
    "# 카메라 연결 확인 코드\n",
    "import cv2\n",
    "\n",
    "for i in range(5):\n",
    "    cap = cv2.VideoCapture(i)\n",
    "    if cap.isOpened():\n",
    "        print(f\"Webcam index {i} is available.\")\n",
    "        cap.release()\n",
    "    else:\n",
    "        print(f\"Webcam index {i} is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alphapose command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Input dir: Run AlphaPose for all images in a folder with:\n",
    "python scripts/demo_inference.py --cfg <cfg_file> --checkpoint <trained_model> --indir <img_directory> --outdir <output_directory>\n",
    "\n",
    "* Choose a different detector: Default detector is yolov3-spp, it works pretty well, if you want to use yolox series, remember to download their weight according to our installation readme. Options include [yolox-x|yolox-l|yolox-m|yolox-s|yolox-darknet]:\n",
    "python scripts/demo_inference.py --detector yolox-x --cfg <cfg_file> --checkpoint <trained_model> --indir <img_directory> --outdir <output_directory>\n",
    "\n",
    "* Video: Run AlphaPose for a video and save the rendered video with:\n",
    "python scripts/demo_inference.py --cfg configs/halpe_26/resnet/256x192_res50_lr1e-3_2x.yaml --checkpoint pretrained_models/halpe26_fast_res50_256x192.pth --video examples/video/cjs_exam.mp4 --outdir examples/results --save_video\n",
    "\n",
    "* Webcam: Run AlphaPose using default webcam and visualize the results with:\n",
    "python scripts/demo_inference.py --cfg <cfg_file> --checkpoint <trained_model> --outdir examples/res --vis --webcam 0\n",
    "\n",
    "* Input list: Run AlphaPose for images in a list and save the rendered images with:\n",
    "python scripts/demo_inference.py --cfg <cfg_file> --checkpoint <trained_model> --list examples/list-coco-demo.txt --indir <img_directory> --outdir examples/res --save_img\n",
    "\n",
    "* Only-cpu/Multi-gpus: Run AlphaPose for images in a list by cpu only or multi gpus:\n",
    "python scripts/demo_inference.py --cfg <cfg_file> --checkpoint <trained_model> --list examples/list-coco-demo.txt --indir <img_directory> --outdir examples/res --gpus ${-1(cpu only)/0,1,2,3(multi-gpus)}\n",
    "\n",
    "* Re-ID Track(Experimental): Run AlphaPose for tracking persons in a video by human re-id algorithm:\n",
    "python scripts/demo_inference.py --cfg <cfg_file> --checkpoint <trained_model> --video <path to video> --outdir examples/res --pose_track --save_video\n",
    "\n",
    "* Simple Track(Experimental): Run AlphaPose for tracking persons in a video by MOT tracking algorithm:\n",
    "python scripts/demo_inference.py --cfg <cfg_file> --checkpoint <trained_model> --video <path to video> --outdir examples/res --detector tracker --save_video\n",
    "\n",
    "* Pose Flow(not ready): Run AlphaPose for tracking persons in a video by embedded PoseFlow algorithm:\n",
    "python scripts/demo_inference.py --cfg <cfg_file> --checkpoint <trained_model> --video <path to video> --outdir examples/res --pose_flow --save_video\n",
    "\n",
    "* Note: If you meet OOM(out of memory) problem, decreasing the pose estimation batch until the program can run on your computer:\n",
    "python scripts/demo_inference.py --cfg <cfg_file> --checkpoint <trained_model> --indir <img_directory> --outdir examples/res --detbatch 1 --posebatch 30\n",
    "\n",
    "* Getting more accurate: You can use larger input for pose network to improve performance e.g.:\n",
    "python scripts/demo_inference.py --cfg <cfg_file> --checkpoint <trained_model> --indir <img_directory> --outdir <output_directory> --flip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data len: 629\n",
      "0.jpg value---------------------------------------\n",
      "Nose [0, 2] : [297.024169921875, 187.48756408691406, 0.9306275248527527]\n",
      "LEye [3, 5] : [330.7660217285156, 164.99301147460938, 0.963004469871521]\n",
      "REye [6, 8] : [274.52960205078125, 164.99301147460938, 0.9414897561073303]\n",
      "LEar [9, 11] : [364.5078430175781, 187.48756408691406, 0.9635714292526245]\n",
      "REar [12, 14] : [240.78778076171875, 187.48756408691406, 0.954016387462616]\n",
      "LShoulder [15, 17] : [454.486083984375, 299.9603576660156, 0.9351562857627869]\n",
      "RShoulder [18, 20] : [218.293212890625, 378.69134521484375, 0.9698046445846558]\n",
      "LElbow [21, 23] : [521.9697875976562, 446.1750183105469, 0.9492277503013611]\n",
      "RElbow [24, 26] : [218.293212890625, 569.8950805664062, 0.9547001719474792]\n",
      "LWrist [27, 29] : [521.9697875976562, 592.3896484375, 0.8853586912155151]\n",
      "RWrist [30, 32] : [207.04592895507812, 727.3569946289062, 0.9393669366836548]\n",
      "LHip [33, 35] : [420.7442626953125, 693.6151733398438, 0.9078469276428223]\n",
      "RHip [36, 38] : [285.7768859863281, 693.6151733398438, 0.9420901536941528]\n",
      "LKnee [39, 41] : [409.4969787597656, 929.8080444335938, 0.9526184797286987]\n",
      "Rknee [42, 44] : [297.024169921875, 929.8080444335938, 0.9052649736404419]\n",
      "LAnkle [45, 47] : [398.24969482421875, 1121.0118408203125, 0.9156575798988342]\n",
      "RAnkle [48, 50] : [308.2714538574219, 1109.7645263671875, 0.8976012468338013]\n",
      "Head [51, 53] : [297.024169921875, 86.26204681396484, 0.9559916853904724]\n",
      "Neck [54, 56] : [319.51873779296875, 277.4658203125, 0.9444617033004761]\n",
      "Hip [57, 59] : [364.5078430175781, 682.367919921875, 0.9703693985939026]\n",
      "LBigToe [60, 62] : [409.4969787597656, 1177.2481689453125, 0.9165810346603394]\n",
      "RBigToe [63, 65] : [263.2823486328125, 1177.2481689453125, 0.9644531607627869]\n",
      "LSmallToe [66, 68] : [431.9915466308594, 1177.2481689453125, 0.9239792227745056]\n",
      "RSmallToe [69, 71] : [240.78778076171875, 1166.0009765625, 0.9183158278465271]\n",
      "LHeel [72, 74] : [387.0024108886719, 1132.2591552734375, 0.9117603898048401]\n",
      "RHeel [75, 77] : [319.51873779296875, 1132.2591552734375, 0.910990834236145]\n"
     ]
    }
   ],
   "source": [
    "# 2d keypoints 결과 json 파일 내용 확인 코드\n",
    "import json\n",
    "\n",
    "# npy 디렉토리\n",
    "file_path = 'AlphaPose/examples/results/alphapose-results.json'\n",
    "\n",
    "# npy 파일 읽기\n",
    "with open(file_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(\"data len:\", len(data))\n",
    "\n",
    "# 데이터 확인\n",
    "# 26 body keypoints\n",
    "#     {0,  \"Nose\"},\n",
    "#     {1,  \"LEye\"},\n",
    "#     {2,  \"REye\"},\n",
    "#     {3,  \"LEar\"},\n",
    "#     {4,  \"REar\"},\n",
    "#     {5,  \"LShoulder\"},\n",
    "#     {6,  \"RShoulder\"},\n",
    "#     {7,  \"LElbow\"},\n",
    "#     {8,  \"RElbow\"},\n",
    "#     {9,  \"LWrist\"},\n",
    "#     {10, \"RWrist\"},\n",
    "#     {11, \"LHip\"},\n",
    "#     {12, \"RHip\"},\n",
    "#     {13, \"LKnee\"},\n",
    "#     {14, \"Rknee\"},\n",
    "#     {15, \"LAnkle\"},\n",
    "#     {16, \"RAnkle\"},\n",
    "#     {17,  \"Head\"},\n",
    "#     {18,  \"Neck\"},\n",
    "#     {19,  \"Hip\"},\n",
    "#     {20, \"LBigToe\"},\n",
    "#     {21, \"RBigToe\"},\n",
    "#     {22, \"LSmallToe\"},\n",
    "#     {23, \"RSmallToe\"},\n",
    "#     {24, \"LHeel\"},\n",
    "#     {25, \"RHeel\"},\n",
    "# x1, y1, c1, ... xk, yk, ck (c: 감지 신뢰도)\n",
    "k = ['Nose', 'LEye', 'REye', 'LEar', 'REar', 'LShoulder', 'RShoulder', 'LElbow', 'RElbow', 'LWrist', 'RWrist',\n",
    "     'LHip', 'RHip', 'LKnee', 'Rknee', 'LAnkle', 'RAnkle', 'Head', 'Neck', 'Hip', 'LBigToe', 'RBigToe', 'LSmallToe', 'RSmallToe', 'LHeel', 'RHeel']\n",
    "ex = data[10]['keypoints']\n",
    "print(\"0.jpg value---------------------------------------\")\n",
    "for i in range(26*3):\n",
    "    if i % 3 == 0: print(k[i//3], [i, i+2], \":\", ex[i+0:i+3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def cal_2d_distance(x1, y1, x2, y2):\n",
    "    x = x2 - x1\n",
    "    y = y2 - y1\n",
    "    return math.sqrt(x**2 + y**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data len: 629\n",
      "\n",
      "----------거북목----------\n",
      "turtle_distance: 44.75555419921875\n",
      "\n",
      "---------골반 기울기----------\n",
      "rad: -0.08314091211370711 / abs rad: 0.08314091211370711\n",
      "deg: -4.7636233689835175 / abs_deg: 4.7636233689835175\n",
      "\n",
      "----------어깨 기울기----------\n",
      "rad: -0.3217506706821978 / abs rad: 0.3217506706821978\n",
      "deg: -18.434955485593566 / abs_deg: 18.434955485593566\n",
      "\n",
      "----------경추 자세----------\n",
      "gyeongchu: 44.75555419921875\n",
      "\n",
      "----------하지 길이 차이----------\n",
      "jong_d: 0.0 / hu_d: 12.203921881053901\n"
     ]
    }
   ],
   "source": [
    "# 불균형 검사 코드 in AlphaPose\n",
    "import json\n",
    "\n",
    "file_path = 'AlphaPose/examples/results/alphapose-results.json'\n",
    "\n",
    "with open(file_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(\"data len:\", len(data))\n",
    "\n",
    "# 거북목 증후군 검사\n",
    "print(\"\\n----------거북목----------\")\n",
    "rframe = 0 # 우측면 이미지 or 프레임\n",
    "val = data[rframe]['keypoints']\n",
    "r_ear_x = val[12]\n",
    "r_shoulder_x = val[18]\n",
    "turtle_distance = r_ear_x - r_shoulder_x\n",
    "print(\"turtle_distance:\", turtle_distance)\n",
    "\n",
    "# 골반 기울기 검사\n",
    "print(\"\\n---------골반 기울기----------\")\n",
    "fframe = 0 # 전면 이미지 or 프레임\n",
    "val = data[fframe]['keypoints']\n",
    "l_hip = val[33:35]\n",
    "r_hip = val[36:38]\n",
    "rad = math.atan2(l_hip[1] - r_hip[1], l_hip[0] - r_hip[0])\n",
    "deg = (rad*180)/math.pi\n",
    "print(\"rad:\", rad, \"/ abs rad:\", abs(rad))\n",
    "print(\"deg:\", deg, \"/ abs_deg:\", abs(deg))\n",
    "\n",
    "# 어깨 기울기 검사\n",
    "print(\"\\n----------어깨 기울기----------\")\n",
    "fframe = 10 # 전면 이미지 or 프레임\n",
    "val = data[fframe]['keypoints']\n",
    "l_shoulder = val[15:17]\n",
    "r_shoulder = val[18:20]\n",
    "rad = math.atan2(l_shoulder[1] - r_shoulder[1], l_shoulder[0] - r_shoulder[0])\n",
    "deg = (rad*180)/math.pi\n",
    "print(\"rad:\", rad, \"/ abs rad:\", abs(rad))\n",
    "print(\"deg:\", deg, \"/ abs_deg:\", abs(deg))\n",
    "\n",
    "# 경추 자세 검사(거북목 증후군 검사와 유사)\n",
    "print(\"\\n----------경추 자세----------\")\n",
    "rframe = 0 # 우측면 이미지 or 프레임\n",
    "val = data[rframe]['keypoints']\n",
    "bias = 5 # 귀에서 유양돌기 사이 거리\n",
    "r_ear_x = val[12]\n",
    "uyang_x = val[12] - bias\n",
    "r_shoulder_x = val[18]\n",
    "gyeongchu = r_ear_x - r_shoulder_x\n",
    "print(\"gyeongchu:\", gyeongchu)\n",
    "\n",
    "# 하지 길이 차이 검사\n",
    "print(\"\\n----------하지 길이 차이----------\")\n",
    "fframe = 0 # 전면 이미지 or 프레임\n",
    "val = data[fframe]['keypoints']\n",
    "l_hip = val[33:35]\n",
    "r_hip = val[36:38]\n",
    "l_knee = val[39:41]\n",
    "r_knee = val[42:44]\n",
    "l_ankle = val[45:47]\n",
    "r_ankle = val[48:50]\n",
    "\n",
    "l_jong_d = cal_2d_distance(l_ankle[0], l_ankle[1], l_knee[0], l_knee[1])\n",
    "r_jong_d = cal_2d_distance(r_ankle[0], r_ankle[1], r_knee[0], r_knee[1])\n",
    "l_hu_d = cal_2d_distance(l_hip[0], l_hip[1], l_knee[0], l_knee[1])\n",
    "r_hu_d = cal_2d_distance(r_hip[0], r_hip[1], r_knee[0], r_knee[1])\n",
    "jong_d = abs(l_jong_d - r_jong_d)\n",
    "hu_d = abs(l_hu_d - r_hu_d)\n",
    "print(\"jong_d:\", jong_d, \"/ hu_d:\", hu_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motionbert command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 3d pose\n",
    "python infer_wild.py \\\n",
    "--vid_path examples/AlphaPose_cjs_exam.mp4 \\\n",
    "--json_path examples/alphapose-results.json \\\n",
    "--out_path examples/results\n",
    "\n",
    "parser.add_argument(\"--config\", type=str, default=\"configs/pose3d/MB_ft_h36m_global_lite.yaml\", help=\"Path to the config file.\")\n",
    "parser.add_argument('-e', '--evaluate', default='checkpoint/pose3d/FT_MB_lite_MB_ft_h36m_global_lite/best_epoch.bin', type=str, metavar='FILENAME', help='checkpoint to evaluate (file name)')\n",
    "parser.add_argument('-j', '--json_path', type=str, help='alphapose detection result json path')\n",
    "parser.add_argument('-v', '--vid_path', type=str, help='video path')\n",
    "parser.add_argument('-o', '--out_path', type=str, help='output path')\n",
    "parser.add_argument('--pixel', action='store_true', help='align with pixle coordinates')\n",
    "parser.add_argument('--focus', type=int, default=None, help='target person id')\n",
    "parser.add_argument('--clip_len', type=int, default=243, help='clip length for network input')\n",
    "\n",
    "\n",
    "* mesh\n",
    "python infer_wild_mesh.py \\\n",
    "--vid_path examples/AlphaPose_cjs_exam.mp4 \\\n",
    "--json_path examples/alphapose-results.json \\\n",
    "--out_path examples/results \\\n",
    "--ref_3d_motion_path examples/results/X3D.npy(option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(629, 17, 3)\n",
      "0 frame---------------------------------------\n",
      "0 - root : [-0.1434395   0.16969839  0.        ]\n",
      "1 - RHip : [-0.25752604  0.18429506 -0.01000124]\n",
      "2 - RKnee : [-0.24302873  0.5482167   0.08387277]\n",
      "3 - RAnkle : [-0.22548307  0.8333118   0.20752114]\n",
      "4 - LHip : [-0.03615615  0.1840654   0.02186609]\n",
      "5 - LKnee : [-0.06927957  0.54697     0.12671795]\n",
      "6 - LAnkle : [-0.08741674  0.8364388   0.23261611]\n",
      "7 - torso : [-0.1459696  -0.13192844 -0.0399621 ]\n",
      "8 - neck : [-0.13689786 -0.414737   -0.08197122]\n",
      "9 - nose : [-0.13566147 -0.5573864  -0.1940051 ]\n",
      "10 - head : [-0.15982705 -0.7071955  -0.17124364]\n",
      "11 - LShoulder : [ 0.0296975  -0.34021437 -0.02077453]\n",
      "12 - LElbow : [ 0.09721148 -0.10437821  0.1192563 ]\n",
      "13 - LWrist : [0.04764319 0.06599944 0.01340875]\n",
      "14 - RShoulder : [-0.3006009  -0.34079653 -0.07275216]\n",
      "15 - RElbow : [-0.35476    -0.09653915  0.01704303]\n",
      "16 - RWrist : [-0.33017528  0.08956937 -0.13777421]\n"
     ]
    }
   ],
   "source": [
    "# 3d pose 결과 numpy 파일 내용 확인 코드\n",
    "import numpy as np\n",
    "\n",
    "# npy 디렉토리\n",
    "file_path = 'MotionBERT/examples/results/X3D.npy'\n",
    "\n",
    "# npy 파일 읽기\n",
    "data = np.load(file_path)\n",
    "\n",
    "# 데이터 확인\n",
    "print(data.shape) # (프레임 개수, 키포인트, (x,y,z) 좌표) \n",
    "# print(data)\n",
    "# h36m data 3d keypoints format\n",
    "k = ['root', 'RHip', 'RKnee', 'RAnkle', 'LHip', 'LKnee', 'LAnkle', 'torso', 'neck', 'nose', 'head', 'LShoulder', 'LElbow', 'LWrist', 'RShoulder', 'RElbow', 'RWrist']\n",
    "frame = 0\n",
    "print(frame, \"frame---------------------------------------\")\n",
    "for i, key in enumerate(data[frame]):\n",
    "    print(i, \"-\", k[i], \":\", key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def cal_2d_distance(x1, y1, x2, y2):\n",
    "    x = x2 - x1\n",
    "    y = y2 - y1\n",
    "    return math.sqrt(x**2 + y**2)\n",
    "\n",
    "def cal_3d_distance(p1, p2):\n",
    "    x1, y1, z1 = p1\n",
    "    x2, y2, z2 = p2\n",
    "    \n",
    "    x = x2 - x1\n",
    "    y = y2 - y1\n",
    "    z = z2 - z1\n",
    "    return math.sqrt(x**2 + y**2 + z**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data len: 629\n",
      "\n",
      "----------거북목----------\n",
      "논의 필요\n",
      "\n",
      "---------골반 기울기----------\n",
      "rad: -0.001026848658174631 / abs rad: 0.001026848658174631\n",
      "deg: -0.058834094312078096 / abs_deg: 0.058834094312078096\n",
      "\n",
      "----------어깨 기울기----------\n",
      "rad: -0.3144783797198704 / abs rad: 0.3144783797198704\n",
      "deg: -18.018283906061075 / abs_deg: 18.018283906061075\n",
      "\n",
      "----------경추 자세----------\n",
      "논의 필요\n",
      "\n",
      "----------하지 길이 차이----------\n",
      "jong_d: 0.006687884218901652 / hu_d: 0.008475302947568153\n"
     ]
    }
   ],
   "source": [
    "# 불균형 검사 코드 in AlphaPose\n",
    "import numpy as np\n",
    "\n",
    "file_path = 'MotionBERT/examples/results/X3D.npy'\n",
    "data = np.load(file_path)\n",
    "\n",
    "print(\"data len:\", len(data))\n",
    "\n",
    "# 거북목 증후군 검사\n",
    "print(\"\\n----------거북목----------\")\n",
    "print(\"논의 필요\")\n",
    "# rframe = 0 # 우측면 이미지 or 프레임\n",
    "# val = data[rframe]\n",
    "# r_ear_x = val[12]\n",
    "# r_shoulder_x = val[18]\n",
    "# turtle_distance = r_ear_x - r_shoulder_x\n",
    "# print(\"turtle_distance:\", turtle_distance)\n",
    "\n",
    "# 골반 기울기 검사\n",
    "print(\"\\n---------골반 기울기----------\")\n",
    "fframe = 0 # 전면 이미지 or 프레임\n",
    "val = data[fframe]\n",
    "l_hip = val[4]\n",
    "r_hip = val[1]\n",
    "hip_xz = cal_2d_distance(l_hip[0], l_hip[2], r_hip[0], r_hip[2])\n",
    "rad = math.atan2(l_hip[1] - r_hip[1], hip_xz)\n",
    "deg = (rad*180)/math.pi\n",
    "print(\"rad:\", rad, \"/ abs rad:\", abs(rad))\n",
    "print(\"deg:\", deg, \"/ abs_deg:\", abs(deg))\n",
    "\n",
    "# 어깨 기울기 검사\n",
    "print(\"\\n----------어깨 기울기----------\")\n",
    "fframe = 10 # 전면 이미지 or 프레임\n",
    "val = data[fframe]\n",
    "l_shoulder = val[11]\n",
    "r_shoulder = val[14]\n",
    "shoulder_xz = cal_2d_distance(l_shoulder[0], l_shoulder[2], r_shoulder[0], r_shoulder[2])\n",
    "rad = math.atan2(l_shoulder[1] - r_shoulder[1], shoulder_xz)\n",
    "deg = (rad*180)/math.pi\n",
    "print(\"rad:\", rad, \"/ abs rad:\", abs(rad))\n",
    "print(\"deg:\", deg, \"/ abs_deg:\", abs(deg))\n",
    "\n",
    "# 경추 자세 검사(거북목 증후군 검사와 유사)\n",
    "print(\"\\n----------경추 자세----------\")\n",
    "print(\"논의 필요\")\n",
    "# rframe = 0 # 우측면 이미지 or 프레임\n",
    "# val = data[rframe]['keypoints']\n",
    "# bias = 5 # 귀에서 유양돌기 사이 거리\n",
    "# r_ear_x = val[12]\n",
    "# uyang_x = val[12] - bias\n",
    "# r_shoulder_x = val[18]\n",
    "# gyeongchu = r_ear_x - r_shoulder_x\n",
    "# print(\"gyeongchu:\", gyeongchu)\n",
    "\n",
    "# 하지 길이 차이 검사\n",
    "print(\"\\n----------하지 길이 차이----------\")\n",
    "fframe = 10 # 전면 이미지 or 프레임\n",
    "val = data[fframe]\n",
    "l_hip = val[4]\n",
    "r_hip = val[1]\n",
    "l_knee = val[5]\n",
    "r_knee = val[2]\n",
    "l_ankle = val[6]\n",
    "r_ankle = val[3]\n",
    "\n",
    "l_jong_d = cal_3d_distance(l_ankle, l_knee)\n",
    "r_jong_d = cal_3d_distance(r_ankle, r_knee)\n",
    "l_hu_d = cal_3d_distance(l_hip, l_knee)\n",
    "r_hu_d = cal_3d_distance(r_hip, r_knee)\n",
    "jong_d = abs(l_jong_d - r_jong_d)\n",
    "hu_d = abs(l_hu_d - r_hu_d)\n",
    "print(\"jong_d:\", jong_d, \"/ hu_d:\", hu_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sjd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
