{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# cuda 연결 확인 코드\n",
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alphapose command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Input dir: Run AlphaPose for all images in a folder with:\n",
    "python scripts/demo_inference.py --cfg <cfg_file> --checkpoint <trained_model> --indir <img_directory> --outdir <output_directory>\n",
    "\n",
    "* Choose a different detector: Default detector is yolov3-spp, it works pretty well, if you want to use yolox series, remember to download their weight according to our installation readme. Options include [yolox-x|yolox-l|yolox-m|yolox-s|yolox-darknet]:\n",
    "python scripts/demo_inference.py --detector yolox-x --cfg <cfg_file> --checkpoint <trained_model> --indir <img_directory> --outdir <output_directory>\n",
    "\n",
    "* Video: Run AlphaPose for a video and save the rendered video with:\n",
    "python scripts/demo_inference.py --cfg configs/halpe_26/resnet/256x192_res50_lr1e-3_2x.yaml --checkpoint pretrained_models/halpe26_fast_res50_256x192.pth --video examples/video/cjs_exam.mp4 --outdir examples/results --save_video\n",
    "\n",
    "* Webcam: Run AlphaPose using default webcam and visualize the results with:\n",
    "python scripts/demo_inference.py --cfg <cfg_file> --checkpoint <trained_model> --outdir examples/res --vis --webcam 0\n",
    "\n",
    "* Input list: Run AlphaPose for images in a list and save the rendered images with:\n",
    "python scripts/demo_inference.py --cfg <cfg_file> --checkpoint <trained_model> --list examples/list-coco-demo.txt --indir <img_directory> --outdir examples/res --save_img\n",
    "\n",
    "* Only-cpu/Multi-gpus: Run AlphaPose for images in a list by cpu only or multi gpus:\n",
    "python scripts/demo_inference.py --cfg <cfg_file> --checkpoint <trained_model> --list examples/list-coco-demo.txt --indir <img_directory> --outdir examples/res --gpus ${-1(cpu only)/0,1,2,3(multi-gpus)}\n",
    "\n",
    "* Re-ID Track(Experimental): Run AlphaPose for tracking persons in a video by human re-id algorithm:\n",
    "python scripts/demo_inference.py --cfg <cfg_file> --checkpoint <trained_model> --video <path to video> --outdir examples/res --pose_track --save_video\n",
    "\n",
    "* Simple Track(Experimental): Run AlphaPose for tracking persons in a video by MOT tracking algorithm:\n",
    "python scripts/demo_inference.py --cfg <cfg_file> --checkpoint <trained_model> --video <path to video> --outdir examples/res --detector tracker --save_video\n",
    "\n",
    "* Pose Flow(not ready): Run AlphaPose for tracking persons in a video by embedded PoseFlow algorithm:\n",
    "python scripts/demo_inference.py --cfg <cfg_file> --checkpoint <trained_model> --video <path to video> --outdir examples/res --pose_flow --save_video\n",
    "\n",
    "* Note: If you meet OOM(out of memory) problem, decreasing the pose estimation batch until the program can run on your computer:\n",
    "python scripts/demo_inference.py --cfg <cfg_file> --checkpoint <trained_model> --indir <img_directory> --outdir examples/res --detbatch 1 --posebatch 30\n",
    "\n",
    "* Getting more accurate: You can use larger input for pose network to improve performance e.g.:\n",
    "python scripts/demo_inference.py --cfg <cfg_file> --checkpoint <trained_model> --indir <img_directory> --outdir <output_directory> --flip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motionbert command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 3d pose\n",
    "python infer_wild.py \\\n",
    "--vid_path examples/AlphaPose_cjs_exam.mp4 \\\n",
    "--json_path examples/alphapose-results.json \\\n",
    "--out_path examples/results\n",
    "\n",
    "parser.add_argument(\"--config\", type=str, default=\"configs/pose3d/MB_ft_h36m_global_lite.yaml\", help=\"Path to the config file.\")\n",
    "parser.add_argument('-e', '--evaluate', default='checkpoint/pose3d/FT_MB_lite_MB_ft_h36m_global_lite/best_epoch.bin', type=str, metavar='FILENAME', help='checkpoint to evaluate (file name)')\n",
    "parser.add_argument('-j', '--json_path', type=str, help='alphapose detection result json path')\n",
    "parser.add_argument('-v', '--vid_path', type=str, help='video path')\n",
    "parser.add_argument('-o', '--out_path', type=str, help='output path')\n",
    "parser.add_argument('--pixel', action='store_true', help='align with pixle coordinates')\n",
    "parser.add_argument('--focus', type=int, default=None, help='target person id')\n",
    "parser.add_argument('--clip_len', type=int, default=243, help='clip length for network input')\n",
    "\n",
    "\n",
    "* mesh\n",
    "python infer_wild_mesh.py \\\n",
    "--vid_path examples/AlphaPose_cjs_exam.mp4 \\\n",
    "--json_path examples/alphapose-results.json \\\n",
    "--out_path examples/results \\\n",
    "--ref_3d_motion_path examples/results/X3D.npy(option)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sjd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
